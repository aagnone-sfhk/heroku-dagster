# dagster.yaml: Production instance configuration for Heroku

# Configures the storage backend for run history, event logs, and schedule/sensor state.
# It uses PostgreSQL, which is provisioned as a Heroku add-on.
storage:
  postgres:
    postgres_url:
      env: DAGSTER_DATABASE_URL

# Configures where stdout/stderr from computations are stored.
# Using a cloud-based manager like S3 is mandatory on Heroku due to its
# ephemeral filesystem, which would otherwise cause logs to be lost on dyno restarts.
compute_logs:
  module: dagster_aws.s3.compute_log_manager
  class: S3ComputeLogManager
  config:
    # The S3 bucket name is sourced from an environment variable.
    bucket:
      env: HDRIVE_S3_BUCKET
    # A prefix to organize logs within the bucket.
    prefix: "dagster-compute-logs/"
    # AWS credentials from Heroku environment variables
    aws_access_key_id:
      env: HDRIVE_S3_ACCESS_KEY
    aws_secret_access_key:
      env: HDRIVE_S3_SECRET_KEY

# Configures the run coordinator to queue runs, preventing the system from being
# overwhelmed by too many concurrent executions. This is a best practice for production.
run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator
  config:
    max_concurrent_runs: 10

